---
layout: remark
title: Data Collection and Summarization
subtitle: Statistics I â€” Data Exploration
author: Deepayan Sarkar
mathjax: true
---


```{r opts, echo = FALSE, results = "hide", warning = FALSE, message = FALSE}
opts_chunk$set(cache = TRUE, cache.path='~/knitr-cache/de-data-summary/', autodep = TRUE,
               comment = "", warning = TRUE, message = TRUE,
			   ## engine.path = list(python = "/usr/bin/python3"),
               fig.width = 12, fig.height = 5,
               dev = "svglite", dev.args = list(pointsize = 12),
               knitr.table.format = "html",
			   fig.path='figures/data-summary-')
options(warnPartialMatchDollar = FALSE, width = 80)
suppressWarnings(require(lattice))
```



<div>
$$
\newcommand{\sub}{_}
\newcommand{\set}[1]{\left\lbrace {#1} \right\rbrace}
\newcommand{\nseq}[1]{ {#1}\sub{1}, {#1}\sub{2}, \dotsc, {#1}\sub{n} }
$$
</div>


# Goals

* Where do data come from?

* Can we classify datasets by how they were collected?

* How can we summarize data numerically or graphically?

---

layout: true

# Sources of data

---

* Traditional data types

	* Categorical - nominal / ordered

	* Numeric - discrete / continuous

--

* Data "modes" that more difficult to analyse (but are of increasing interest)

	* Free text
	
	* Images
	
	* Sound
	
	* Many others
	
--

* But how are such data typically collected?

---

* Key methodologies

	* Census
	
	* Sample survey
	
	* Observational studies / Case-control studies
	
	* Randomized studies / Randomized controlled trials


* Key concepts

	* Observational units

	* Population
	
	* Sample
	
---

layout: true

# Some scenarios

---

* We will consider some specific scenarios along with a question of interest

* In each case, we want to plan a data collection experiment that will answer the question 

---

* What is the leading cause of death for adults in India?

	* Does it vary by sex?
	
	* Does it vary by state? By district?
	
	* Does it vary by education level? By income?
	
	* Do the answers change over time?
	
--

* What is the target population?

* Can we do a census?

* Can we do a survey?

* How should we sample?

* What are the observational units?

* Observational or randomized? Controls?

---

* How much money does a typical adult in India make?

	* Does it vary by sex?
	
	* Does it vary by state? By district?
	
	* Does it vary by education level? By employment status?
	
	* Do the answers change over time?
	
--

* What is the target population?

* Can we do a census?

* Can we do a survey?

* How should we sample?

* What are the observational units?

* Observational or randomized? Controls?

---

* Consider a "random" person who will be born in India in 2025

* How likely is it that they will be born on January 1? January 2? ... December 31?

--

* What is the target population?

* Can we do a census?

* Can we do a survey?

* How should we sample?

* What are the observational units?

* Observational or randomized? Controls?

--

* Here is a [relevant dataset](https://www.ons.gov.uk/visualisations/nesscontent/dvc307/line_chart/data.csv)  (CSV) from [England and Wales](https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/articles/howpopularisyourbirthday/2015-12-18)

* Can we use this data to answer the same question for England and Wales?

--

* Similar but more important question: Predict the __air quality__ in Delhi from October to December?

---

* A medicine factory produces tablets that are supposed to have a certain chemical composition

	* A regulator wants to check whether the actual composition is within acceptable limits

	* However, the only way to test a tablet is "destructive", i.e., the tablet cannot be used afterward

--

* What is the target population?

* Can we do a census?

* Can we do a survey?

* How should we sample?

* What are the observational units?

* Observational or randomized? Controls?

---

* A new yearly injection can potentially delay the onset of diabetes

	* However, both the amount of benefit and the possible side effects are currently unknown

	* We want to plan an experiment to study both

--

* What is the target population?

* Can we do a census?

* Can we do a survey?

* How should we sample?

* What are the observational units?

* Observational or randomized? Controls?

---

* Your friend loves a particular type of pizza from Dominos

* He also claims he can differentiate between the pizzas from two different outlets in your neighbourhood

* You want to test whether this claim is true

--

* What is the target population?

* Can we do a census?

* Can we do a survey?

* How should we sample?

* What are the observational units?

* Observational or randomized? Controls?

---

* What is the GDP of India? What is the average life expectancy in India?

* How do they compare with other contries?

* How have they changed over time?

--

* How do we measure?

* More basic question: how are they defined?

--

* These are by definition __summary__ measures

* Example: [TSV data](https://deepayan.github.io/BSDS/2024-01-DE/data/gapminder.tsv) and [visualization](https://www.gapminder.org/tools/) from GapMinder

* The actual process to calculate these from unit-level data is probably quite complicated


---

layout: true

# What do we actually do with data?

---

* The process of data analysis is often involves just calculating various summary measures

--

* In fact, the technical name for a summary measure computed from data is a __statistic__

* In this course, we will mostly learn about commonly used summary statistics

--

* But also important to remember that summary statistics are usually computed from _samples_

* We need to be careful when we use them to make conclusions about a larger _population_

---

* Example: Are birthdays equally likely? 

* Let $X$ be the smallest birth month frequency in a 'random sample' of $n = 65$ people

* We take one sample, where we observe $X = 3$. What can we conclude?

--

```{r}
min_count <- function(n, pmonths = NULL)
{
    b <- sample(1:12, n, prob = pmonths, replace = TRUE)
    T <- table(b)
    if (length(T) < 12) {
        return(0)
    }
    else {
        return(min(T))
    }
}
```

---

```{r}
replicate(10000, min_count(65)) |>
    table() |> prop.table() |> barplot()
```


---

```{r}
replicate(10000, min_count(65, pmonths = c(31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31) / 365)) |>
    table() |> prop.table() |> barplot()
```

---

* Example: Are birthdays equally likely? 

* Let $X$ be the smallest birth month frequency in a 'random sample' of $n = 65$ people

* We take one sample, where we observe $X = 3$. What can we conclude?

--

* We __cannot__ conclude that birthdays are _not_ equally likely

---

layout: true

# What statistics are useful?

---

* This is a natural and important question

* The answer depends on the type of data and the problem of interest

---

* How can we summarize categorical variables such as _cause of death_?

--

	* We usually want to find the probabilities of various categories

	* Natural summary statistic is sample proportion
	
	* The 'most likely' category is known as the __mode__

---

* How can we summarize numeric variables such as _income_?

	* This is much more difficult to answer
	
	* Common summary statistics: __mean__ and __median__
	
	* But these capture only limited aspects of the distribution
	
--

* We will learn about mean, median and similar summary statistics

* But first we will learn about the _empirical distribution_


---

layout: true

# Empirical distribution

---

* Suppose we have observed values $\nseq{X}$ of some variable

* Assume that the values of $\nseq{X}$ are all _distinct_

* The empirical distribution is a probability distribution whose

	* Sample space is the set $\set{ \nseq{X} }$
	
	* All elements in the sample space are equally likely (has probability $1/n$)
	
--

* This is adjusted suitably if values are not distinct

	* The sample space consists of all distinct values

	* If the $i$th value appears $k\sub{i}$ times, the corresponding probability if $k\sub{i} / n$

---

* The empirical distribution contains all information in the data...

	* as long as the observations are "independent" (a 'random sample')
	
* As it is a probability distribution, we can apply the tools we have to study distributions

--

* Why is the empirical distribution important?

	* We have seen that sample proportions 'converge' to probabilities as sample size $n$ increases

	* More generally, the empirical distribution also 'converges' to the population distribution

--
	
	* This is one of the _main justifications_ for using observed data to 'infer' about underlying population

--

	* This convergence is mathematically more complicated (which will not be discussed in this course)
	
	* However, understanding this convergence graphically is important 

---

layout: true

# Visualizing the empirical distribution

---

* Visualizing the empirical distribution is important but challenging

--

* Let us consider the height data we collected in our survey

```{r}
survey <- read.csv("https://deepayan.github.io/BSDS/2024-01-DE/data/bsds-survey.csv")
height <- survey$height
height
```

* How do we visualize it?

---

```{r}
stripchart(height, method = "stack")
```

---

* For comparison, let us simulate data uniformly between 150 and 200

```{r}
n <- length(height)
u <- sample(150:195, n, replace = TRUE)
stripchart(list(unif = u, obs = height), method = "stack")
```

---

* Are the two samples similar or very different? Not very easy to say

--

* Another possible way to compare

```{r}
par(mfrow = c(1, 2)); plot(u,       ylim = c(150, 195)); plot(height,       ylim = c(150, 195))
```

---

* There seems to be some qualitative difference

* This difference becomes clearer if we plot _sorted_ data

```{r}
par(mfrow = c(1, 2)); plot(sort(u), ylim = c(150, 195)); plot(sort(height), ylim = c(150, 195))
```

---

* It is more traditional to swap the axes and convert the y-axis to a probability scale

```{r}
par(mfrow = c(1, 2)); plot(sort(u), ppoints(n), xlim = c(150, 195));
                      plot(sort(height), ppoints(n), xlim = c(150, 195))
```

---

* This is equivalent to the _empirical cumulative distribution function_ (ECDF)

```{r}
par(mfrow = c(1, 2)); plot(ecdf(u), xlim = c(150, 195));
                      plot(ecdf(height), xlim = c(150, 195))
```

---

layout: true

# The empirical cumulative distribution function (ECDF)

---

* Height data from a different source (the NHANES survey)

```{r}
library(NHANES)
dim(NHANES)
nhsub1 <- subset(NHANES, !is.na(Height)) # remove missing height values
n1 <- nrow(nhsub1)
n1
```

---

* ECDF of height of a random sample of Americans

```{r}
plot(sort(nhsub1$Height), ppoints(n1), type = "l")
```

---

* Low heights are from children

* We are interested in the height distribution among adults

```{r}
nhsub2 <- subset(nhsub1, Age >= 20) # remove children
n2 <- nrow(nhsub2)
n2
```

---

* ECDF of height of a random sample of American adults

```{r}
plot(sort(nhsub2$Height), ppoints(n2), type = "l")
```

---

* ECDF of the magic distribution: Normal

```{r}
plot(sort(rnorm(n2)), ppoints(n2), type = "l")
```

---

layout: true

# Quantile-Quantile plots

---

* These ECDF plots essentially show sorted data vs equally spaced probabilities

--

* Useful alternative: plot sorted data against sorted data from the 'Normal distribution'

```{r}
par(mfrow = c(1, 2)); plot(sort(rnorm(n2)), sort(nhsub2$Height))
```

---

* How do we get data from the 'Normal distribution'? Simulate using `rnorm()`

* Problem: Every time we simulate, we will get a slightly different plot

```{r}
par(mfrow = c(1, 2)); plot(sort(rnorm(n2)), sort(nhsub2$Height));
```

---

* Solution: Instead of simulating, use something called 'theoretical quantiles' of the Normal distribution

* The result is known as a _Normal Quantile-Quantile Plot_ or simply _Normal Q-Q plot_

```{r}
par(mfrow = c(1, 2)); plot(sort(rnorm(n2)), sort(nhsub2$Height)); qqnorm(nhsub2$Height)
```

---

* As before, the lattice package has a different implementation that is more powerful

```{r}
qqmath(~ Height, data = nhsub2, grid = TRUE, aspect = 1, groups = Gender, auto.key = TRUE)
```

---

layout: true

# Questions

---

* This raises several important questions:

	* What is the Normal distribution and why should we compare with it?

	* What are quantiles?

	* What are theoretical quantiles?

	* What can Q-Q plots tell us?
	
--

* The first two questions will be discussed in your probability course

* We will give very vague answers for now

---

* What is the Normal distribution

	* The Normal distribution is a very simple _continuous_ distribution
	
	* It is best for now to think of it as an _approximation_ to various distributions that we observe in real life

	* We will see examples of this soon

---

* What are quantiles?

	* In Q-Q plots we plot sorted data
	
	* Sorted data have a special name in statistics: _order statistics_
	
--

	* Specifically, the $k$-th value of $X\sub{i}$ in sorted order is known as the $k$-th order statistic
	
	* The standard notation for the $k$-th order statistic is $X\sub{(k)}$

--

	* Roughly speaking, quantiles are order statistics, but identified by their _relative rank_ (like percentiles)
	
---

* Examples of quantiles:

	* The $90$-th percentile is the same as the $0.9$ quantile
	
	* This is defined as the number $Q$ such that 90% of the data is less than (or equal to) $Q$
	
	* In a sample of size $n = 100$, this would be $X\sub{(90)}$

	* In a sample of size $n = 1000$, this would be $X\sub{(900)}$

--

	* In a sample of size $n = 7182$, this would be ?? 

--

```{r}
0.9 * 7182
```

---
	
* Several approximations available; see `help(quantile)`
	
```{r}
with(subset(nhsub2, Gender == "male"), quantile(Height, prob = c(0.2, 0.4, 0.6, 0.8)))
with(subset(nhsub2, Gender == "female"), quantile(Height, prob = c(0.2, 0.4, 0.6, 0.8)))
```

---

* What are theoretical quantiles?

	* Similar idea, but with probability instead of relative rank
	
	* $p$-th quantile is the number $Q$ such that $P(X \leq Q) = p$
	
	* Needs to be modified suitably to account for discrete jumps
	
--

* R has _quantile functions_ for all standard distributions

```{r}
qbinom(c(0.2, 0.4, 0.6, 0.8), size = 20, prob = 0.25)
qbinom(c(0.2, 0.4, 0.6, 0.8), size = 200, prob = 0.25)
qnorm(c(0.2, 0.4, 0.6, 0.8))
```

---

* Why are Q-Q plots useful?

	* Sample quantiles converge to theoretical quantiles of underlying population
	
	* Q-Q plots compare order statistics with corresponding theoretical quantiles 

--

* Why not ECDF?

	* This has to do with _human perception_
	
	* We find it easier to detect departures from a straight line (as opposed to a curve)

--
	
	* In principle, the theoretical quantiles can be from any appropriate distribution

	* The Normal distribution has been found to be the most useful default choice

---




---


layout: false
class: center, middle

# Questions?





